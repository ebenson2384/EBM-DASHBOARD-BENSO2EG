# Assessment & Monitoring Plan
# How we will measure success and adapt over time

## Logic Model: Theory of Change

Our evidence-based intervention follows this causal pathway:

### X → M → Y Framework

**X (Intervention Components):**
- Communication Foundations Workshop (4-hour onboarding training)
- Weekly "Sync & Clarify" Check-ins (15-min structured touchpoints)
- Communication Resource Hub (self-service knowledge base)

**M (Mediating Mechanisms - How it works):**
1. **Knowledge & Skills**: Interns learn effective communication frameworks (5W1H, norms, channels)
2. **Reinforcement Loops**: Weekly check-ins prevent skill decay and catch issues early
3. **Psychological Safety**: Explicit permission to ask questions reduces fear barriers
4. **Accessibility**: Resource hub enables self-service, reducing bottlenecks and wait times

**Y (Expected Outcomes):**
- **Primary**: 30-40% reduction in communication-related incidents (from 8.2/month to 5-6/month)
- **Secondary**: Manager time savings (from 6 hrs/week to 3 hrs/week on clarifications)
- **Tertiary**: Improved intern confidence in communication (from 42% to 80% "confident asking questions")

### Causal Logic
- **IF** interns gain communication skills and norms (M1) + receive ongoing reinforcement (M2) + feel safe asking questions (M3) + have easy access to resources (M4)
- **THEN** they will ask better questions, use appropriate channels, and resolve issues faster
- **RESULTING IN** fewer miscommunications, less manager time spent on clarifications, and better project outcomes

---

## Evaluation Design: Interrupted Time Series with Comparison

### Why This Design?
Based on our evidence synthesis:
- **Scientific evidence** shows interrupted time series is more rigorous than simple before/after (controls for trends)
- **Organizational constraints** prevent randomized controlled trial (can't ethically withhold training from some interns)
- **Practitioner best practice** recommends multiple measurement points to detect decay effects

### Design Structure

**Timeline:**
```
Baseline Period (3 months): Oct-Dec 2025
  ↓
Intervention Launch: January 2026
  ↓
Post-Intervention Monitoring: Jan-Jun 2026 (monthly measurements)
```

**Treatment Group**: Current and future intern cohorts (n=12-16 per cohort)

**Comparison Group (optional enhancement)**: Historical data from previous 12 months (Oct 2024-Sep 2025) to establish secular trends

### Data Collection Points
- **Baseline**: 3 monthly measurements (Oct, Nov, Dec 2025)
- **Post-Intervention**: Monthly measurements for 6 months (Jan-Jun 2026)
- **Follow-up**: Quarterly measurements in Year 2 to assess sustainability

### What We're Measuring
See KPI section below for specific metrics.

---

## Key Performance Indicators (KPIs)

### Primary Outcome KPIs

**KPI 1: Communication Incident Rate**
- **Definition**: Number of communication-related incidents per month (missed deadlines, rework due to misunderstanding, escalations to senior staff)
- **Baseline**: 8.2 incidents/month (based on manager logs Oct-Dec 2025)
- **Target**: 5-6 incidents/month by Month 3 (30-40% reduction)
- **Data Source**: Manager incident logs (standardized tracking form)
- **Frequency**: Monthly
- **Responsible**: Managers submit logs to HR by 5th of each month

**KPI 2: Manager Time Spent on Clarifications**
- **Definition**: Hours per week managers spend answering repetitive/basic clarification questions
- **Baseline**: 6 hours/week (based on manager time-tracking survey)
- **Target**: 3 hours/week by Month 3 (50% reduction)
- **Data Source**: Weekly manager time-tracking logs
- **Frequency**: Weekly (reported monthly)
- **Responsible**: Managers log time in shared spreadsheet

**KPI 3: Intern Communication Confidence**
- **Definition**: % of interns who report feeling "confident or very confident" asking questions and clarifying expectations
- **Baseline**: 42% (based on current intern survey)
- **Target**: 80% by Month 3
- **Data Source**: Monthly pulse survey (2-minute Likert scale)
- **Frequency**: Monthly
- **Responsible**: HR sends survey, analyzes results

### Secondary Outcome KPIs

**KPI 4: Resource Hub Self-Service Rate**
- **Definition**: % of common questions resolved via hub search (vs. asking a person)
- **Baseline**: 0% (hub doesn't exist yet)
- **Target**: 50% by Month 6
- **Data Source**: Hub search analytics + manager logs of "could have been in hub" questions
- **Frequency**: Monthly
- **Responsible**: HR reviews analytics

**KPI 5: Project Quality Score**
- **Definition**: Average quality rating of intern deliverables (scored by managers on 1-5 scale)
- **Baseline**: 3.2/5 (based on Q4 2025 project reviews)
- **Target**: 4.0/5 by Month 6
- **Data Source**: Manager project evaluation forms
- **Frequency**: Per project (aggregated monthly)
- **Responsible**: Managers submit evaluations at project completion

### Process/Implementation KPIs

**KPI 6: Workshop Satisfaction & Learning**
- **Definition**: (a) % of interns satisfied with workshop (4-5 on 5-point scale); (b) Knowledge gain on pre/post quiz
- **Target**: (a) 85% satisfaction; (b) 40% average improvement on quiz
- **Data Source**: Post-workshop survey + pre/post knowledge assessment
- **Frequency**: Per workshop session (quarterly)
- **Responsible**: HR facilitator

**KPI 7: Check-in Completion Rate**
- **Definition**: % of scheduled weekly check-ins that actually occur
- **Target**: 95% completion rate
- **Data Source**: Manager calendar compliance logs
- **Frequency**: Weekly (reported monthly)
- **Responsible**: HR reviews calendar data

**KPI 8: Resource Hub Usage**
- **Definition**: (a) Number of unique users per month; (b) Average searches per user; (c) Content quality rating
- **Target**: (a) 100% of interns use hub at least once; (b) 3+ searches per user per month; (c) 4.0/5 content rating
- **Data Source**: Hub analytics + quarterly feedback survey
- **Frequency**: Monthly analytics, quarterly survey
- **Responsible**: HR hub owner

---

## Implementation Fidelity Monitoring

To ensure the intervention is delivered as designed (critical for interpreting results):

### Fidelity Dimensions

**1. Content Fidelity: Are we teaching the right things?**
- **Standard**: Workshop covers all 5 core modules (norms, 5W1H, safety, pitfalls, tools)
- **Monitoring**: Observer checklist during workshops (HR director spot-checks 25% of sessions)
- **Action if below standard**: Re-train facilitator, update workshop materials

**2. Dose Fidelity: Are we delivering enough?**
- **Standard**: Workshop is full 4 hours (not shortened); check-ins are full 15 minutes
- **Monitoring**: Workshop duration logged; check-in calendar time blocks reviewed
- **Action if below standard**: Block workshop time more firmly; remind managers to protect check-in time

**3. Moderator Fidelity: Are contextual factors stable?**
- **Standard**: No major organizational changes (restructuring, layoffs, policy shifts) during evaluation period
- **Monitoring**: Monthly check-in with leadership on organizational changes
- **Action if changes occur**: Document in evaluation report; extend evaluation period if needed

---

## Decision Framework: What Do We Do with the Results?

### Decision Rules (at 6-month evaluation point)

**Scenario 1: Strong Success**
- **Criteria**: Primary KPIs meet or exceed targets (incidents ≤5.5/month, manager time ≤3 hrs/week, confidence ≥80%)
- **Decision**: **CONTINUE & SCALE**
- **Actions**: 
  - Maintain all three intervention components
  - Share success story with leadership; request budget for expansion
  - Begin planning advanced modules (e.g., conflict resolution, cross-cultural communication)

**Scenario 2: Partial Success**
- **Criteria**: 1-2 primary KPIs improve significantly, but not all targets met
- **Decision**: **MODIFY & CONTINUE**
- **Actions**:
  - Analyze which component(s) aren't working (check process KPIs)
  - If workshop satisfaction low → revise content based on feedback
  - If check-in completion low → address manager barriers (time, buy-in)
  - If hub usage low → improve discoverability, add more content
  - Re-evaluate at 12 months after modifications

**Scenario 3: No Improvement**
- **Criteria**: No significant change in primary KPIs after 6 months
- **Decision**: **PAUSE & DIAGNOSE**
- **Actions**:
  - Check implementation fidelity (was intervention delivered as designed?)
  - If fidelity low → Re-launch with better adherence
  - If fidelity high but no results → Return to evidence synthesis; consider alternative explanations (cultural issues more dominant than skills gaps)
  - Conduct stakeholder interviews to understand barriers

**Scenario 4: Negative Effects**
- **Criteria**: Outcomes worsen (incidents increase, manager time increases, confidence drops)
- **Decision**: **CANCEL INTERVENTION**
- **Actions**:
  - Immediately stop resource-intensive components (free up manager time)
  - Maintain only hub as low-cost resource
  - Investigate unintended consequences (e.g., did check-ins create learned helplessness?)

---

## Data Collection Timeline & Responsibilities

| Phase | Time Period | Data Collection Activities | Responsible Party |
|-------|-------------|---------------------------|-------------------|
| **Baseline** | Oct-Dec 2025 | Incident logs, manager time surveys, intern confidence survey | Managers + HR |
| **Launch** | Jan 2026 | Workshop pre/post quiz, satisfaction survey | HR facilitator |
| **Month 1** | Feb 2026 | All KPIs (incidents, time, confidence, check-in completion) | Managers + HR |
| **Month 2** | Mar 2026 | All KPIs + hub usage begins | Managers + HR |
| **Month 3** | Apr 2026 | All KPIs + project quality ratings | Managers + HR |
| **Month 6** | Jul 2026 | **FORMAL EVALUATION**: All KPIs + stakeholder interviews | External evaluator or HR lead |
| **Year 2** | Quarterly | Sustainability check: KPIs 1-3 only (reduced frequency) | HR |

---

## Potential Confounds & How We'll Address Them

**Confound 1: Seasonal variation**
- **Issue**: Intern performance may naturally improve over time as they gain general job experience
- **Mitigation**: Compare to historical data from previous cohorts; use comparison group if possible

**Confound 2: Selection effects**
- **Issue**: Future cohorts may be stronger/weaker than current cohort for unrelated reasons
- **Mitigation**: Track cohort demographics and prior experience; statistically control if needed

**Confound 3: Hawthorne effect**
- **Issue**: Improvements may result from increased attention, not the intervention itself
- **Mitigation**: Acknowledge this in evaluation report; monitor for decay effects in Year 2

**Confound 4: External events**
- **Issue**: Company-wide policy changes, market conditions, or other factors may impact results
- **Mitigation**: Document all organizational changes; include in evaluation narrative

---

## Ethical Considerations

**Equity**: All interns receive the intervention (no control group denied services)
**Privacy**: Survey data collected confidentially; only aggregated results shared
**Transparency**: Interns informed that program is being evaluated and their feedback matters
**Consent**: Participation in surveys is voluntary (though workshop/check-ins are mandatory job requirements)

---

## Reporting & Communication Plan

### Internal Reporting
- **Monthly Dashboard**: One-page summary of KPIs 1-3, shared with managers and leadership
- **Quarterly Deep Dive**: Detailed analysis of all KPIs, shared with project team
- **6-Month Formal Evaluation**: Comprehensive report with recommendations, presented to leadership

### Stakeholder Communication
- **Managers**: Monthly email with KPI trends + actionable insights ("check-in completion dropped to 80% this month—let's troubleshoot")
- **Interns**: Quarterly "You Said, We Did" summary showing how their feedback shaped improvements
- **Leadership**: 6-month presentation with ROI analysis and recommendations for scaling

---

## Long-Term Monitoring (Years 2-3)

After initial evaluation, shift to **sustainability monitoring**:

- **Frequency**: Quarterly instead of monthly
- **Metrics**: Focus on KPIs 1-3 only (incidents, manager time, confidence)
- **Purpose**: Detect decay effects; ensure intervention remains embedded in practice
- **Adaptations**: Annual review of workshop content and hub resources to keep current

---

## Conclusion

This assessment plan ensures we will:
1. **Know if the intervention works** (primary KPIs)
2. **Understand why it works or doesn't** (process KPIs + fidelity monitoring)
3. **Make data-driven decisions** (clear decision rules)
4. **Adapt over time** (quarterly reviews, long-term monitoring)

By using an interrupted time series design with multiple measurement points and implementation fidelity checks, we go beyond simple "did it work?" to answer "how well did it work, under what conditions, and should we continue?"

This rigorous approach aligns with the evidence-based management principles guiding our entire project.
